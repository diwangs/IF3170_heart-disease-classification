{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize path constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "RAW_DATA_PATH = '{}/raw'.format(DATA_PATH)\n",
    "PROCESSED_DATA_PATH = '{}/processed'.format(DATA_PATH)\n",
    "\n",
    "MODEL_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read processed data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('{}/processed_data.csv'.format(PROCESSED_DATA_PATH))\n",
    "X = data.drop(['heart_disease_diagnosis','sex','num_of_major_vessels'], axis=1)\n",
    "y = data['heart_disease_diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_data_partition(X, y):\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_pred=y_pred, y_true=y_true)\n",
    "    precision = precision_score(y_pred=y_pred, y_true=y_true, average='macro')\n",
    "    recall = recall_score(y_pred=y_pred, y_true=y_true, average='macro')\n",
    "    \n",
    "    print('--- Validation Metrics ---')\n",
    "    print('Accuracy  = {:.3f}'.format(accuracy))\n",
    "    print('Precision = {:.3f}'.format(precision))\n",
    "    print('Recall    = {:.3f}'.format(recall))\n",
    "\n",
    "def print_cv_result(model, X, y):\n",
    "    # Partition data\n",
    "    X_train, X_validation, y_train, y_validation = get_cv_data_partition(X, y)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict validation\n",
    "    prediction = model.predict(X_validation)\n",
    "    \n",
    "    # Print validation metrics\n",
    "    print_metrics(y_validation, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes ===\n",
      "\n",
      "--- Validation Metrics ---\n",
      "Accuracy  = 0.571\n",
      "Precision = 0.408\n",
      "Recall    = 0.432\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Naive Bayes'\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "print('=== {} ===\\n'.format(model_name))\n",
    "print_cv_result(nb_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors algorithm, it's really important to scale the features first (feature scaling).\n",
    "Since the range of values of raw data varies widely, in K-Nearest Neighbors algoritm, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== K-Nearest Neighbor ===\n",
      "\n",
      "--- Validation Metrics ---\n",
      "Accuracy  = 0.545\n",
      "Precision = 0.387\n",
      "Recall    = 0.327\n"
     ]
    }
   ],
   "source": [
    "model_name = 'K-Nearest Neighbor'\n",
    "knn_model = KNeighborsClassifier()\n",
    "X_Scaled = preprocessing.scale(X)\n",
    "\n",
    "print('=== {} ===\\n'.format(model_name))\n",
    "print_cv_result(knn_model, X_Scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Tree ===\n",
      "\n",
      "--- Validation Metrics ---\n",
      "Accuracy  = 0.455\n",
      "Precision = 0.335\n",
      "Recall    = 0.321\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Decision Tree'\n",
    "dtc_model = DecisionTreeClassifier(criterion='entropy', random_state=1)\n",
    "\n",
    "print('=== {} ===\\n'.format(model_name))\n",
    "print_cv_result(dtc_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANN ===\n",
      "\n",
      "Iteration 1, loss = 1.44603137\n",
      "Iteration 2, loss = 1.36179690\n",
      "Iteration 3, loss = 1.31641627\n",
      "Iteration 4, loss = 1.28611168\n",
      "Iteration 5, loss = 1.26701383\n",
      "Iteration 6, loss = 1.25047268\n",
      "Iteration 7, loss = 1.23935273\n",
      "Iteration 8, loss = 1.23155788\n",
      "Iteration 9, loss = 1.22591125\n",
      "Iteration 10, loss = 1.22150550\n",
      "Iteration 11, loss = 1.21664599\n",
      "Iteration 12, loss = 1.21147469\n",
      "Iteration 13, loss = 1.20848573\n",
      "Iteration 14, loss = 1.20550590\n",
      "Iteration 15, loss = 1.20034981\n",
      "Iteration 16, loss = 1.19591337\n",
      "Iteration 17, loss = 1.19271237\n",
      "Iteration 18, loss = 1.18916824\n",
      "Iteration 19, loss = 1.18239896\n",
      "Iteration 20, loss = 1.17976465\n",
      "Iteration 21, loss = 1.18163414\n",
      "Iteration 22, loss = 1.17997689\n",
      "Iteration 23, loss = 1.17493432\n",
      "Iteration 24, loss = 1.17041438\n",
      "Iteration 25, loss = 1.16788758\n",
      "Iteration 26, loss = 1.16317273\n",
      "Iteration 27, loss = 1.16441787\n",
      "Iteration 28, loss = 1.16055251\n",
      "Iteration 29, loss = 1.15655046\n",
      "Iteration 30, loss = 1.15507686\n",
      "Iteration 31, loss = 1.15293015\n",
      "Iteration 32, loss = 1.15054848\n",
      "Iteration 33, loss = 1.15120104\n",
      "Iteration 34, loss = 1.15435569\n",
      "Iteration 35, loss = 1.14746522\n",
      "Iteration 36, loss = 1.14711398\n",
      "Iteration 37, loss = 1.14072380\n",
      "Iteration 38, loss = 1.14748223\n",
      "Iteration 39, loss = 1.14366466\n",
      "Iteration 40, loss = 1.14454815\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "--- Validation Metrics ---\n",
      "Accuracy  = 0.596\n",
      "Precision = 0.431\n",
      "Recall    = 0.317\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ANN'\n",
    "ann_model = MLPClassifier(random_state=1, alpha=0.00210, epsilon=1e-8, activation='logistic', verbose=True)\n",
    "\n",
    "print('=== {} ===\\n'.format(model_name))\n",
    "print_cv_result(ann_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ann_model\n",
    "pickle.dump(best_model, open('{}/best_model.pkl'.format(MODEL_PATH), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
